{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1jLJ12ldQs6uCL_hYlDrXRflWuC_KZAF8","authorship_tag":"ABX9TyNX2WWH4AMsl8TWjBk41OBv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"metadata":{"id":"l20Rj1HP7LB-"},"cell_type":"code","source":["# Base Imports\n","import numpy as np\n","from google.colab import files\n","import os\n","from skimage.transform import resize\n","from imageio import imread\n","import datetime\n","import os\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing existing model\n","from keras.models import load_model\n","trans_model = load_model(\"old_trained_keras_model.h5\")"],"metadata":{"id":"HPQTDLtaE4pU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Seeding randomly. Prevents results from varying drastically\n","np.random.seed(30)\n","import random as rn\n","rn.seed(30)\n","from keras import backend as K\n","import tensorflow as tf\n","tf.random.set_seed(30)"],"metadata":{"id":"ygZSQ_6VWHcm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_doc = np.random.permutation(open('/content/drive/MyDrive/thumbs/train.csv').readlines())\n","val_doc = np.random.permutation(open('/content/drive/MyDrive/thumbs/val.csv').readlines())\n","batch_size = 32 # Reduce if server throws an error"],"metadata":{"id":"Z3VIndXEWj6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Func Defs: Image preprocessing\n","\n","def cropResize(image, y, z):\n","    h, w = image.shape\n","    \n","    # if smaller image crop at center for 120x120\n","    if w == 160:\n","        image = image[:120, 20:140]\n","\n","    # resize every image\n","    return resize(image, (y,z))\n","\n","def normalizeImage(image):\n","    # applying normalization\n","    return image/255.0\n","\n","def preprocessImage(image, y, z):\n","    return normalizeImage(cropResize(image, y, z))\n","\n","def make2dFilter(x):\n","    return tuple([x]*2)"],"metadata":{"id":"S8EjMyqPZ_kC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getBatchData(source_path, t, batch, batch_size, img_tensor):\n","    [x,y,z] = [len(img_tensor[0]),img_tensor[1], img_tensor[2]]\n","    img_idx = img_tensor[0]\n","    batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n","    batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n","    for folder in range(batch_size): # iterate over the batch_size\n","        imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n","        for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n","            image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","\n","            #crop the images and resize them. Note that the images are of 2 different shape \n","\n","            # separate preprocessImage function is defined for cropping, resizing and normalizing images\n","            batch_data[folder,idx,:,:,0] = preprocessImage(image[:, :, 0], y, z)\n","            batch_data[folder,idx,:,:,1] = preprocessImage(image[:, :, 1], y, z)\n","            batch_data[folder,idx,:,:,2] = preprocessImage(image[:, :, 2], y, z)\n","\n","        batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n","    print(\"debug\", batch)\n","    return batch_data, batch_labels"],"metadata":{"id":"7iyT8ayJannA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generator(source_path, folder_list, batch_size, img_tensor):\n","    print( 'Source path = ', source_path, '; batch size =', batch_size)\n","    while True:\n","        t = np.random.permutation(folder_list)\n","        num_batches = int(len(folder_list)/batch_size)\n","        print(\"words are!\",num_batches)\n","        for batch in range(num_batches): # we iterate over the number of batches\n","            yield getBatchData(source_path, t, batch, batch_size, img_tensor)\n","        \n","        # write the code for the remaining data points which are left after full batches\n","        # checking if any remaining batches are there or not\n","        print(\"I am here!\")\n","        if len(folder_list)%batch_size != 0:\n","            # updated the batch size and yield\n","            batch_size = len(folder_list)%batch_size\n","            yield getBatchData(source_path, t, batch, batch_size, img_tensor)"],"metadata":{"id":"flPDLp0XbRRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["curr_dt_time = datetime.datetime.now()\n","train_path = '/content/drive/MyDrive/thumbs/train'\n","val_path = '/content/drive/MyDrive/thumbs/val'\n","num_train_sequences = len(train_doc)\n","print('# training sequences =', num_train_sequences)\n","num_val_sequences = len(val_doc)\n","print('# validation sequences =', num_val_sequences)\n","num_epochs = 90\n","print ('# epochs =', num_epochs)"],"metadata":{"id":"TQTPoV06bToB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getImgTensor(n_frames):\n","    img_idx = np.round(np.linspace(0, 29, n_frames)).astype(int)\n","    return [img_idx, 100, 100, 3]\n","\n","# define image tensor size\n","img_tensor = getImgTensor(20)\n","print ('# img_tensor =', img_tensor)"],"metadata":{"id":"Fd8JGZESbyzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check complete batch shape\n","sample_generator = generator(train_path, train_doc, batch_size, img_tensor)\n","sample_batch_data, sample_batch_labels = next(sample_generator)\n","print(sample_batch_data.shape)\n","\n","# Validation batch sample\n","sample_val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n","sample_val_batch_data, sample_val_batch_labels = next(sample_val_generator)\n","print(sample_val_batch_data.shape)"],"metadata":{"id":"_MrLzFhvb4ch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot generated sample images\n","fig, ax = plt.subplots(1,2)\n","ax[0].imshow(sample_val_batch_data[16,10,:,:,:])   \n","ax[1].imshow(sample_val_batch_data[25,10,:,:,:])\n","plt.show()"],"metadata":{"id":"kvvHzpi4b9-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotModelHistory(h):\n","    fig, ax = plt.subplots(1, 2, figsize=(15,4))\n","    ax[0].plot(h.history['loss'])   \n","    ax[0].plot(h.history['val_loss'])\n","    ax[0].legend(['loss','val_loss'])\n","    ax[0].title.set_text(\"Train loss vs Validation loss\")\n","\n","    ax[1].plot(h.history['categorical_accuracy'])   \n","    ax[1].plot(h.history['val_categorical_accuracy'])\n","    ax[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n","    ax[1].title.set_text(\"Train accuracy vs Validation accuracy\")\n","    plt.show()\n","\n","    print(\"Max. Training Accuracy\", max(h.history['categorical_accuracy']))\n","    print(\"Max. Validaiton Accuracy\", max(h.history['val_categorical_accuracy']))"],"metadata":{"id":"woFv5cphcE37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Imports\n","from tensorflow import keras\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, LSTM\n","from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras import optimizers"],"metadata":{"id":"EPyEl9V3cfzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if (num_train_sequences%batch_size) == 0:\n","    steps_per_epoch = int(num_train_sequences/batch_size)\n","else:\n","    steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","if (num_val_sequences%batch_size) == 0:\n","    validation_steps = int(num_val_sequences/batch_size)\n","else:\n","    validation_steps = (num_val_sequences//batch_size) + 1"],"metadata":{"id":"emf5xVz_c-Nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n","    \n","if not os.path.exists(model_name):\n","    os.mkdir(model_name)\n","\n","filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","\n","LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n","\n","# callbacks_list = [checkpoint, LR]\n","callbacks_list = [LR]"],"metadata":{"id":"Mkz-ai7yfpVS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_frames = 20\n","num_epochs = 60\n","batch_size = 32\n","\n","img_tensor = getImgTensor(n_frames)\n","train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n","val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n","\n","inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])"],"metadata":{"id":"SqQ6wU3VipjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Starts Here\n","def defineModel():\n","  model = Sequential([\n","      TimeDistributed(Conv2D(16, make2dFilter(3), padding='same', activation='relu'), input_shape=inputShape),\n","      TimeDistributed(BatchNormalization()),\n","      TimeDistributed(MaxPooling2D(make2dFilter(2))),\n","\n","      TimeDistributed(Conv2D(32, make2dFilter(3), padding='same', activation='relu')),\n","      TimeDistributed(BatchNormalization()),\n","      TimeDistributed(MaxPooling2D(make2dFilter(2))),\n","\n","      TimeDistributed(Conv2D(64, make2dFilter(3), padding='same', activation='relu')),\n","      TimeDistributed(BatchNormalization()),\n","      TimeDistributed(MaxPooling2D(make2dFilter(2))),\n","\n","      TimeDistributed(Conv2D(128, make2dFilter(3), padding='same', activation='relu')),\n","      TimeDistributed(BatchNormalization()),\n","      TimeDistributed(MaxPooling2D(make2dFilter(2))),\n","\n","      TimeDistributed(Conv2D(256, make2dFilter(3), padding='same', activation='relu')),\n","      TimeDistributed(BatchNormalization()),\n","      TimeDistributed(MaxPooling2D(make2dFilter(2))),\n","\n","      TimeDistributed(Flatten()),\n","      LSTM(256),\n","      Dropout(0.2),\n","\n","      Dense(256, activation='relu'),\n","      Dropout(0.2),\n","\n","      Dense(2, activation='softmax')\n","  ], name=\"conv_2d_lstm\")\n","  model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","  return model"],"metadata":{"id":"wOxIubTlct7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile model as Transfer from previous gestures.\n","for i in range(5):\n","    trans_model.layers[i].trainable = False\n","\n","for i in range(5,9):\n","    trans_model.layers[i].trainable = True\n","\n","tm = trans_model.layers[8].output\n","tm = Dense(128)(tm)\n","tm = Dense(64)(tm)\n","tm = Dense(2,activation=\"softmax\")(tm)\n","\n","model = Model(inputs=trans_model.input,outputs=tm)\n","\n","model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"],"metadata":{"id":"OFnFe82W37yV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile and Obtain Base model.\n","model = defineModel()"],"metadata":{"id":"Kj-lYsya32cF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summary \n","model.summary()"],"metadata":{"id":"zANq79tS4INI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if (num_train_sequences%batch_size) == 0:\n","    steps_per_epoch = int(num_train_sequences/batch_size)\n","else:\n","    steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","if (num_val_sequences%batch_size) == 0:\n","    validation_steps = int(num_val_sequences/batch_size)\n","else:\n","    validation_steps = (num_val_sequences//batch_size) + 1\n","\n","model = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n","            callbacks=callbacks_list, validation_data=val_generator, \n","            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"],"metadata":{"id":"R2RjWtz-3QIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('trained_keras_model.h5')"],"metadata":{"id":"_f359px27US-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plotModelHistory(model)"],"metadata":{"id":"XexciSfPd9Oo"},"execution_count":null,"outputs":[]}]}